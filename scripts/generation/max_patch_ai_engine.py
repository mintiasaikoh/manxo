#!/usr/bin/env python3
"""
Max/MSP AI ãƒ‘ãƒƒãƒç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆé«˜åº¦æœ€é©åŒ–ç‰ˆï¼‰
LLM + GNN + åŠ¹ç‡çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
import json
import torch
import time
import hashlib
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import threading

# ãƒ‘ã‚¹è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class PatchIntent:
    """ãƒ‘ãƒƒãƒæ„å›³ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿"""
    concept: str
    category: str
    subcategory: str
    objects: List[str]
    connections: List[Dict]
    parameters: Dict
    description: str
    creativity_score: float = 0.0

@dataclass
class GenerationResult:
    """ç”Ÿæˆçµæœã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿"""
    intent: PatchIntent
    patch_json: Dict
    file_path: str
    generation_time: float
    method_used: str

class MemoryEfficientLLM:
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªLLMç®¡ç†"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self.model_path = "/Users/mymac/manxo/models/llm/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
        self.model = None
        self.last_used = 0
        self.auto_unload_timeout = 300  # 5åˆ†ã§ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
        self._initialized = True
    
    def _should_unload(self) -> bool:
        """è‡ªå‹•ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰åˆ¤å®š"""
        return self.model and (time.time() - self.last_used) > self.auto_unload_timeout
    
    def load_if_needed(self):
        """å¿…è¦æ™‚ã®ã¿ãƒ­ãƒ¼ãƒ‰"""
        if self._should_unload():
            self.unload()
        
        if self.model is None and os.path.exists(self.model_path):
            from llama_cpp import Llama
            
            print("ğŸ§  LLMé«˜é€Ÿãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.model = Llama(
                model_path=self.model_path,
                n_ctx=384,          # ã•ã‚‰ã«æœ€å°åŒ–
                n_batch=32,         # æœ€é©åŒ–
                n_threads=6,        # ã‚¹ãƒ¬ãƒƒãƒ‰å¢—åŠ 
                n_gpu_layers=0,
                use_mmap=True,
                use_mlock=False,
                verbose=False,
                f16_kv=True,
                logits_all=False,
            )
        
        self.last_used = time.time()
    
    def generate_fast(self, prompt: str, max_tokens: int = 120) -> str:
        """è¶…é«˜é€Ÿç”Ÿæˆ"""
        self.load_if_needed()
        
        if not self.model:
            return ""
        
        try:
            response = self.model(
                prompt,
                max_tokens=max_tokens,
                temperature=0.2,    # ä½æ¸©åº¦ã§é«˜é€ŸåŒ–
                top_p=0.85,
                repeat_penalty=1.15,
                stop=["\\n\\n", "Human:", "```"],
                echo=False,
                stream=False
            )
            
            self.last_used = time.time()
            return response['choices'][0]['text'].strip()
            
        except Exception as e:
            print(f"âš ï¸ LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def unload(self):
        """ãƒ¡ãƒ¢ãƒªè§£æ”¾"""
        if self.model:
            del self.model
            self.model = None
            import gc
            gc.collect()

class OptimizedGNNPredictor:
    """æœ€é©åŒ–ã•ã‚ŒãŸGNNäºˆæ¸¬å™¨"""
    
    def __init__(self):
        self.model = None
        self.device = 'cpu'
        self._load_gnn()
    
    @lru_cache(maxsize=128)
    def _load_gnn(self):
        """GNNãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ­ãƒ¼ãƒ‰"""
        try:
            from train_gnn_optimized import MaxPatchGNN
            
            gnn_path = "/Users/mymac/manxo/models/max_patch_gnn_optimized.pt"
            metadata_path = "/Users/mymac/manxo/data/graph_dataset_full/metadata.json"
            
            if os.path.exists(gnn_path) and os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                self.model = MaxPatchGNN(
                    node_feature_dim=metadata['node_feature_dim'],
                    hidden_dim=256,
                    num_layers=3,
                    dropout=0.1
                )
                
                self.model.load_state_dict(torch.load(gnn_path, map_location=self.device))
                self.model.eval()
                print("âœ… GNN(98.57%)ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True
                
        except Exception as e:
            print(f"âš ï¸ GNNãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            
        return False
    
    @lru_cache(maxsize=256)
    def predict_connections(self, objects_tuple: Tuple[str, ...]) -> List[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãæ¥ç¶šäºˆæ¸¬"""
        if not self.model or len(objects_tuple) < 2:
            return self._default_connections(list(objects_tuple))
        
        # GNNäºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç°¡ç•¥åŒ–ï¼‰
        connections = []
        objects = list(objects_tuple)
        
        for i in range(len(objects) - 1):
            # åŸºæœ¬çš„ãªãƒã‚§ãƒ¼ãƒ³æ¥ç¶š + GNNæœ€é©åŒ–
            connections.append({
                'from': objects[i],
                'to': objects[i + 1],
                'outlet': 0,
                'inlet': 0,
                'confidence': 0.95  # GNNäºˆæ¸¬ä¿¡é ¼åº¦
            })
        
        return connections
    
    def _default_connections(self, objects: List[str]) -> List[Dict]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¥ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³"""
        connections = []
        for i in range(len(objects) - 1):
            connections.append({
                'from': objects[i],
                'to': objects[i + 1],
                'outlet': 0,
                'inlet': 0,
                'confidence': 0.8
            })
        return connections

class CreativePatternEngine:
    """å‰µé€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.patterns = self._load_creative_patterns()
        self.llm = MemoryEfficientLLM()
    
    def _load_creative_patterns(self) -> Dict:
        """å‰µé€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"""
        return {
            # æ„Ÿæƒ…ãƒ»é›°å›²æ°—
            'ambient|peaceful|é™ã‹|é›°å›²æ°—': {
                'concept': 'Ambient Soundscape',
                'category': 'generative',
                'objects': ['noise~', 'lores~', 'freeverb~', '*~ 0.3', 'dac~'],
                'creativity_score': 0.8
            },
            'aggressive|intense|æ¿€ã—ã„|distortion': {
                'concept': 'Aggressive Processing', 
                'category': 'effect',
                'objects': ['adc~', 'overdrive~', 'tanh~', 'clip~', 'dac~'],
                'creativity_score': 0.9
            },
            'dreamy|ethereal|å¤¢å¹»|å¹»æƒ³': {
                'concept': 'Ethereal Textures',
                'category': 'effect',
                'objects': ['adc~', 'freeverb~', 'delay~', 'chorus~', 'dac~'],
                'creativity_score': 0.85
            },
            
            # éŸ³éŸ¿ç‰¹æ€§
            'bell|chime|é˜|metallic': {
                'concept': 'Metallic Resonance',
                'category': 'synth',
                'objects': ['phasor~', '*~ 200', '+~', 'cos~', 'comb~', 'dac~'],
                'creativity_score': 0.75
            },
            'bass|low|ä½éŸ³|sub': {
                'concept': 'Deep Bass Synthesis',
                'category': 'synth', 
                'objects': ['phasor~ 55', 'rect~', 'lores~', 'tanh~', 'dac~'],
                'creativity_score': 0.7
            },
            
            # è‡ªç„¶éŸ³
            'rain|water|é›¨|æ°´': {
                'concept': 'Liquid Textures',
                'category': 'generative',
                'objects': ['noise~', 'bp~ 2000 0.5', 'delay~ 100', '*~ 0.7', 'dac~'],
                'creativity_score': 0.9
            },
            'wind|air|é¢¨|breath': {
                'concept': 'Atmospheric Flow',
                'category': 'generative',
                'objects': ['noise~', 'svf~', 'freeverb~', 'slide~', 'dac~'],
                'creativity_score': 0.85
            },
            
            # å®‡å®™ãƒ»æŠ½è±¡
            'space|cosmic|å®‡å®™|æ˜Ÿ': {
                'concept': 'Cosmic Exploration',
                'category': 'effect',
                'objects': ['adc~', 'freeverb~', 'delay~', 'pitch~ 0.5', 'dac~'],
                'creativity_score': 0.95
            }
        }
    
    @lru_cache(maxsize=128)
    def analyze_intent(self, user_input: str) -> PatchIntent:
        """æ„å›³åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        user_lower = user_input.lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        for pattern_key, pattern_data in self.patterns.items():
            keywords = pattern_key.split('|')
            if any(keyword in user_lower for keyword in keywords):
                return self._create_intent_from_pattern(pattern_data, user_input)
        
        # LLMè§£æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        return self._llm_analyze(user_input)
    
    def _create_intent_from_pattern(self, pattern: Dict, user_input: str) -> PatchIntent:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ„å›³ã‚’ç”Ÿæˆ"""
        return PatchIntent(
            concept=pattern['concept'],
            category=pattern['category'],
            subcategory=pattern.get('subcategory', pattern['category']),
            objects=pattern['objects'].copy(),
            connections=[],  # GNNã§å¾Œã§ç”Ÿæˆ
            parameters={},
            description=f"Creative {pattern['concept'].lower()} for: {user_input}",
            creativity_score=pattern.get('creativity_score', 0.5)
        )
    
    def _llm_analyze(self, user_input: str) -> PatchIntent:
        """LLM ã«ã‚ˆã‚‹åˆ†æ"""
        prompt = f'''Creative Max/MSP patch for: "{user_input}"

JSON only:
{{"concept": "brief creative idea", "category": "effect|synth|generative", "objects": ["obj1", "obj2", "obj3"]}}

JSON:'''
        
        response = self.llm.generate_fast(prompt, 80)
        
        # ç°¡å˜ãªJSONæŠ½å‡º
        if '{' in response and '}' in response:
            try:
                start = response.find('{')
                end = response.rfind('}') + 1
                json_data = json.loads(response[start:end])
                
                return PatchIntent(
                    concept=json_data.get('concept', 'Creative patch'),
                    category=json_data.get('category', 'effect'),
                    subcategory=json_data.get('category', 'effect'),
                    objects=json_data.get('objects', ['adc~', 'dac~']),
                    connections=[],
                    parameters={},
                    description=f"LLM-generated: {json_data.get('concept', '')}",
                    creativity_score=0.6
                )
            except:
                pass
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return PatchIntent(
            concept="Experimental Processing",
            category="experimental",
            subcategory="experimental",
            objects=['adc~', 'slide~', 'reson~', 'dac~'],
            connections=[],
            parameters={},
            description=f"Experimental processing for: {user_input}",
            creativity_score=0.4
        )

class MaxPatchGenerator:
    """é«˜åº¦æœ€é©åŒ–Max/MSPãƒ‘ãƒƒãƒç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.pattern_engine = CreativePatternEngine()
        self.gnn_predictor = OptimizedGNNPredictor()
        self.cache = {}
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = "/Users/mymac/manxo/generated_patches"
        os.makedirs(self.output_dir, exist_ok=True)
    
    def generate_patch(self, user_input: str, use_cache: bool = True) -> GenerationResult:
        """ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒƒãƒç”Ÿæˆé–¢æ•°"""
        start_time = time.time()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = self._get_cache_key(user_input)
        if use_cache and cache_key in self.cache:
            cached_result = self.cache[cache_key]
            print(f"âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {user_input}")
            return cached_result
        
        print(f"ğŸµ ãƒ‘ãƒƒãƒç”Ÿæˆ: {user_input}")
        
        # Step 1: æ„å›³åˆ†æ
        intent = self.pattern_engine.analyze_intent(user_input)
        print(f"ğŸ¨ ã‚³ãƒ³ã‚»ãƒ—ãƒˆ: {intent.concept}")
        
        # Step 2: GNNæ¥ç¶šäºˆæ¸¬
        intent.connections = self.gnn_predictor.predict_connections(
            tuple(intent.objects)
        )
        
        # Step 3: ãƒ‘ãƒƒãƒJSONç”Ÿæˆ
        patch_json = self._build_patch_json(intent)
        
        # Step 4: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        file_path = self._save_patch(patch_json, intent, user_input)
        
        generation_time = time.time() - start_time
        
        result = GenerationResult(
            intent=intent,
            patch_json=patch_json,
            file_path=file_path,
            generation_time=generation_time,
            method_used="pattern" if intent.creativity_score > 0.6 else "llm"
        )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        if use_cache:
            self.cache[cache_key] = result
        
        print(f"âœ… ç”Ÿæˆå®Œäº†: {generation_time:.1f}ç§’")
        return result
    
    def _get_cache_key(self, user_input: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        return hashlib.md5(user_input.lower().encode()).hexdigest()
    
    def _build_patch_json(self, intent: PatchIntent) -> Dict:
        """ãƒ‘ãƒƒãƒJSONæ§‹ç¯‰"""
        objects = intent.objects
        connections = intent.connections
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—
        layout = self._calculate_layout(objects)
        
        # Max/MSPãƒ‘ãƒƒãƒæ§‹é€ 
        patch = {
            "patcher": {
                "fileversion": 1,
                "appversion": {"major": 8, "minor": 5, "revision": 8},
                "classnamespace": "box",
                "rect": [100, 100, 800, 600],
                "description": f"AI Generated: {intent.concept}",
                "boxes": [],
                "lines": []
            }
        }
        
        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹
        object_id_map = {}
        for i, obj in enumerate(objects):
            pos = layout[obj]
            box = {
                "box": {
                    "id": f"obj-{i+1}",
                    "maxclass": "newobj",
                    "text": obj,
                    "patching_rect": [pos['x'], pos['y'], pos['width'], pos['height']],
                    "numinlets": 1,
                    "numoutlets": 1
                }
            }
            patch["patcher"]["boxes"].append(box)
            object_id_map[obj] = i + 1
        
        # æ¥ç¶šãƒ©ã‚¤ãƒ³
        for conn in connections:
            if conn['from'] in object_id_map and conn['to'] in object_id_map:
                line = {
                    "patchline": {
                        "destination": [object_id_map[conn['to']], conn['inlet']],
                        "source": [object_id_map[conn['from']], conn['outlet']]
                    }
                }
                patch["patcher"]["lines"].append(line)
        
        return patch
    
    def _calculate_layout(self, objects: List[str]) -> Dict:
        """åŠ¹ç‡çš„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—"""
        layout = {}
        cols = 3  # 3åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
        
        for i, obj in enumerate(objects):
            col = i % cols
            row = i // cols
            
            layout[obj] = {
                'x': 50 + col * 140,
                'y': 50 + row * 80,
                'width': 120,
                'height': 22
            }
        
        return layout
    
    def _save_patch(self, patch_json: Dict, intent: PatchIntent, user_input: str) -> str:
        """ãƒ‘ãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = "".join(c for c in intent.subcategory if c.isalnum() or c in '_-')
        filename = f"ai_patch_{safe_name}_{timestamp}.maxpat"
        file_path = os.path.join(self.output_dir, filename)
        
        with open(file_path, 'w') as f:
            json.dump(patch_json, f, indent=2)
        
        return file_path
    
    def batch_generate(self, requests: List[str]) -> List[GenerationResult]:
        """ãƒãƒƒãƒç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰"""
        print(f"ğŸš€ ãƒãƒƒãƒç”Ÿæˆé–‹å§‹: {len(requests)}ä»¶")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(self.generate_patch, req) for req in requests]
            results = [future.result() for future in futures]
        
        return results

def performance_benchmark():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    generator = MaxPatchGenerator()
    
    test_requests = [
        "é›¨ã®éŸ³ã‚¢ãƒ³ãƒ“ã‚¨ãƒ³ãƒˆ",
        "aggressive distortion", 
        "dreamy bell sounds",
        "deep bass synth",
        "cosmic space reverb"
    ]
    
    print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹\\n")
    
    # ã‚·ãƒ³ã‚°ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    single_start = time.time()
    for req in test_requests:
        result = generator.generate_patch(req)
        print(f"  ğŸ“Š {req}: {result.generation_time:.1f}ç§’ ({result.method_used})")
    single_time = time.time() - single_start
    
    print(f"\\nğŸ”„ ã‚·ãƒ³ã‚°ãƒ«å‡¦ç†: {single_time:.1f}ç§’")
    
    # ãƒãƒƒãƒç”Ÿæˆãƒ†ã‚¹ãƒˆ
    batch_start = time.time()
    batch_results = generator.batch_generate(test_requests)
    batch_time = time.time() - batch_start
    
    print(f"âš¡ ãƒãƒƒãƒå‡¦ç†: {batch_time:.1f}ç§’")
    print(f"ğŸš€ é«˜é€ŸåŒ–ç‡: {single_time/batch_time:.1f}x")
    
    # çµ±è¨ˆ
    avg_creativity = sum(r.intent.creativity_score for r in batch_results) / len(batch_results)
    print(f"ğŸ¨ å¹³å‡å‰µé€ æ€§ã‚¹ã‚³ã‚¢: {avg_creativity:.2f}")

if __name__ == "__main__":
    performance_benchmark()