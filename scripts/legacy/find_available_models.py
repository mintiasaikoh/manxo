#!/usr/bin/env python3
"""
åˆ©ç”¨å¯èƒ½ãªGGUFãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
"""

from huggingface_hub import list_repo_files

def list_available_models():
    """åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º"""
    
    repos = [
        "Qwen/Qwen2.5-14B-Instruct-GGUF",
        "Qwen/Qwen2.5-7B-Instruct-GGUF", 
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-large"
    ]
    
    for repo in repos:
        try:
            print(f"\nğŸ“ {repo}:")
            files = list_repo_files(repo)
            
            # GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤º
            gguf_files = [f for f in files if f.endswith('.gguf')]
            
            if gguf_files:
                for file in gguf_files[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
                    print(f"  âœ… {file}")
                if len(gguf_files) > 5:
                    print(f"  ... and {len(gguf_files) - 5} more files")
            else:
                print("  âŒ No GGUF files found")
                
        except Exception as e:
            print(f"  âŒ Error accessing {repo}: {e}")

def download_working_model():
    """å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    from huggingface_hub import hf_hub_download
    import os
    
    model_dir = "/Users/mymac/manxo/models/llm"
    os.makedirs(model_dir, exist_ok=True)
    
    # ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
    alternatives = [
        {
            "repo": "microsoft/DialoGPT-medium", 
            "file": "pytorch_model.bin",
            "size": "1.4GB",
            "description": "DialogGPT Medium (å¯¾è©±ç‰¹åŒ–)"
        },
        {
            "repo": "TheBloke/Llama-2-7B-Chat-GGUF",
            "file": "llama-2-7b-chat.Q4_K_M.gguf", 
            "size": "4.1GB",
            "description": "Llama2 7B Chat Q4"
        },
        {
            "repo": "TheBloke/CodeLlama-7B-Instruct-GGUF",
            "file": "codellama-7b-instruct.Q4_K_M.gguf",
            "size": "4.2GB", 
            "description": "CodeLlama 7B Instruct Q4"
        }
    ]
    
    for alt in alternatives:
        try:
            print(f"\nğŸ”„ {alt['description']} ã‚’è©¦è¡Œä¸­...")
            print(f"ã‚µã‚¤ã‚º: {alt['size']}")
            
            model_path = hf_hub_download(
                repo_id=alt["repo"],
                filename=alt["file"],
                local_dir=model_dir
            )
            
            print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {model_path}")
            return model_path
            
        except Exception as e:
            print(f"âŒ {alt['description']} å¤±æ•—: {e}")
            continue
    
    return None

if __name__ == "__main__":
    print("ğŸ” åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ä¸­...")
    list_available_models()
    
    print("\nğŸš€ å®Ÿç”¨çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model_path = download_working_model()
    
    if model_path:
        print(f"\nğŸ‰ ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†: {model_path}")
    else:
        print("\nâŒ å…¨ã¦ã®ä»£æ›¿æ¡ˆãŒå¤±æ•—ã—ã¾ã—ãŸ")