#!/usr/bin/env python3
"""
GGUFãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ãƒ‡ãƒãƒƒã‚°
"""

import os
from llama_cpp import Llama

def test_simple_load():
    """æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®šã§ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
    
    model_path = "/Users/mymac/manxo/models/llm/qwen2.5-7b-instruct-q4_k_m.gguf"
    
    if not os.path.exists(model_path):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")
        return False
    
    file_size = os.path.getsize(model_path) / (1024**3)
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {file_size:.1f}GB")
    
    try:
        print("ğŸ§  æœ€å°è¨­å®šã§ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ...")
        model = Llama(
            model_path=model_path,
            n_ctx=256,      # æœ€å°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            n_batch=8,      # æœ€å°ãƒãƒƒãƒ
            n_gpu_layers=0, # CPUå®Ÿè¡Œ
            use_mlock=False,
            use_mmap=True,
            verbose=True    # è©³ç´°ãƒ­ã‚°
        )
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ!")
        
        # æœ€å°ãƒ†ã‚¹ãƒˆ
        response = model("Hello", max_tokens=5, echo=False)
        print(f"ğŸ“ å¿œç­”: {response}")
        
        del model
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        return False

def test_alternative_models():
    """ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ"""
    
    model_dir = "/Users/mymac/manxo/models/llm"
    
    # åˆ©ç”¨å¯èƒ½ãªGGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¨ã¦è©¦ã™
    gguf_files = [f for f in os.listdir(model_dir) if f.endswith('.gguf')]
    
    print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªGGUFãƒ•ã‚¡ã‚¤ãƒ«: {len(gguf_files)}å€‹")
    
    for gguf_file in gguf_files:
        if gguf_file == "qwen2.5-14b-instruct-q4_k_m.gguf":
            continue  # 0GBãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            
        model_path = os.path.join(model_dir, gguf_file)
        file_size = os.path.getsize(model_path) / (1024**3)
        
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆ: {gguf_file} ({file_size:.1f}GB)")
        
        if file_size < 0.1:  # 100MBæœªæº€ã¯ã‚¹ã‚­ãƒƒãƒ—
            print("â¸ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™")
            continue
        
        try:
            model = Llama(
                model_path=model_path,
                n_ctx=256,
                n_gpu_layers=0,
                verbose=False
            )
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆ
            response = model("Create Max/MSP reverb:", max_tokens=20, echo=False)
            print(f"âœ… æˆåŠŸ: {response['choices'][0]['text'][:50]}...")
            
            del model
            import gc
            gc.collect()
            
            return model_path  # æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™
            
        except Exception as e:
            print(f"âŒ å¤±æ•—: {str(e)[:100]}...")
            continue
    
    return None

def download_confirmed_working_model():
    """ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    
    from huggingface_hub import hf_hub_download
    model_dir = "/Users/mymac/manxo/models/llm"
    
    # ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹å°ã•ãªãƒ¢ãƒ‡ãƒ«
    working_models = [
        {
            "repo": "microsoft/DialoGPT-small",
            "file": "pytorch_model.bin",
            "description": "DialoGPT Small (117MB)",
            "use_transformers": True
        },
        {
            "repo": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
            "file": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf", 
            "description": "TinyLlama 1.1B (669MB)",
            "use_transformers": False
        }
    ]
    
    for model_config in working_models:
        try:
            print(f"â¬‡ï¸ {model_config['description']} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            model_path = hf_hub_download(
                repo_id=model_config["repo"],
                filename=model_config["file"],
                local_dir=model_dir
            )
            
            file_size = os.path.getsize(model_path) / (1024**2)  # MB
            print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {file_size:.0f}MB")
            
            if not model_config["use_transformers"]:
                # GGUFå½¢å¼ãªã®ã§llama-cppã§ãƒ†ã‚¹ãƒˆ
                model = Llama(model_path=model_path, n_ctx=256, verbose=False)
                response = model("Hello", max_tokens=10, echo=False)
                print(f"âœ… å‹•ä½œç¢ºèª: {response['choices'][0]['text']}")
                del model
            
            return model_path
            
        except Exception as e:
            print(f"âŒ {model_config['description']} å¤±æ•—: {e}")
            continue
    
    return None

if __name__ == "__main__":
    print("ğŸ”§ GGUFãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ãƒ‡ãƒãƒƒã‚°é–‹å§‹...")
    
    # Step 1: çµåˆã—ãŸQwenãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    print("\\n=== Step 1: Qwençµåˆãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ ===")
    qwen_success = test_simple_load()
    
    if qwen_success:
        print("ğŸ‰ Qwenãƒ¢ãƒ‡ãƒ«æ­£å¸¸å‹•ä½œ!")
    else:
        # Step 2: ä»£æ›¿GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
        print("\\n=== Step 2: ä»£æ›¿GGUFãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ ===")
        working_model = test_alternative_models()
        
        if working_model:
            print(f"ğŸ‰ å‹•ä½œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹: {os.path.basename(working_model)}")
        else:
            # Step 3: ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            print("\\n=== Step 3: å‹•ä½œç¢ºèªæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ===")
            fallback_model = download_confirmed_working_model()
            
            if fallback_model:
                print(f"ğŸ‰ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†: {os.path.basename(fallback_model)}")
            else:
                print("âŒ å…¨ã¦ã®é¸æŠè‚¢ãŒå¤±æ•—ã—ã¾ã—ãŸ")
    
    print("\\nğŸ”§ ãƒ‡ãƒãƒƒã‚°å®Œäº†")