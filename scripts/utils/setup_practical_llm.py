#!/usr/bin/env python3
"""
å®Ÿç”¨çš„ãªLLMãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºå®Ÿã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
"""

from huggingface_hub import hf_hub_download, list_repo_files
import os

def find_and_download_qwen():
    """åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    
    model_dir = "/Users/mymac/manxo/models/llm"
    os.makedirs(model_dir, exist_ok=True)
    
    # Qwen 7Bã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    try:
        print("ğŸ” Qwen2.5-7B ã®åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªä¸­...")
        files = list_repo_files("Qwen/Qwen2.5-7B-Instruct-GGUF")
        
        # Q4é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        q4_files = [f for f in files if 'q4' in f.lower() and f.endswith('.gguf')]
        print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªQ4ãƒ•ã‚¡ã‚¤ãƒ«: {q4_files[:3]}")
        
        if q4_files:
            # æœ€åˆã®Q4ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            target_file = q4_files[0]
            print(f"â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {target_file}")
            
            model_path = hf_hub_download(
                repo_id="Qwen/Qwen2.5-7B-Instruct-GGUF",
                filename=target_file,
                local_dir=model_dir
            )
            
            file_size = os.path.getsize(model_path) / (1024**3)
            print(f"âœ… Qwen ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {file_size:.1f}GB")
            return model_path
            
    except Exception as e:
        print(f"âŒ Qwen ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    
    return None

def download_alternative_model():
    """ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    
    model_dir = "/Users/mymac/manxo/models/llm" 
    
    # ç¢ºå®Ÿã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«
    alternatives = [
        {
            "repo": "TheBloke/Llama-2-7B-Chat-GGUF",
            "file": "llama-2-7b-chat.Q4_K_M.gguf",
            "description": "Llama2 7B Chat (4.1GB)"
        },
        {
            "repo": "TheBloke/CodeLlama-7B-Instruct-GGUF", 
            "file": "codellama-7b-instruct.Q4_K_M.gguf",
            "description": "CodeLlama 7B (4.2GB) - ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ç‰¹åŒ–"
        }
    ]
    
    for alt in alternatives:
        try:
            print(f"â¬‡ï¸ {alt['description']} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            model_path = hf_hub_download(
                repo_id=alt["repo"],
                filename=alt["file"],
                local_dir=model_dir
            )
            
            file_size = os.path.getsize(model_path) / (1024**3)
            print(f"âœ… {alt['description']} å®Œäº†: {file_size:.1f}GB")
            return model_path
            
        except Exception as e:
            print(f"âŒ {alt['description']} å¤±æ•—: {e}")
            continue
    
    return None

def test_any_model(model_path):
    """ä»»æ„ã®GGUFãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    if not model_path or not os.path.exists(model_path):
        return False
    
    try:
        from llama_cpp import Llama
        
        print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {os.path.basename(model_path)}")
        model = Llama(
            model_path=model_path,
            n_ctx=512,  # å°ã•ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            n_gpu_layers=0,
            verbose=False
        )
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚¹ãƒˆ
        response = model(
            "Create a Max/MSP reverb effect using:",
            max_tokens=50,
            echo=False,
            temperature=0.3
        )
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèªæˆåŠŸ!")
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆå¿œç­”: {response['choices'][0]['text'][:100]}...")
        
        del model
        import gc
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def create_llm_interface():
    """LLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ"""
    
    interface_code = '''#!/usr/bin/env python3
"""
Max/MSP ãƒ‘ãƒƒãƒç”Ÿæˆç”¨ LLM ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import os
import json
from llama_cpp import Llama

class MaxPatchLLM:
    def __init__(self, model_path=None):
        if model_path is None:
            # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
            model_dir = "/Users/mymac/manxo/models/llm"
            gguf_files = [f for f in os.listdir(model_dir) if f.endswith('.gguf')]
            if gguf_files:
                model_path = os.path.join(model_dir, gguf_files[0])
            else:
                raise FileNotFoundError("GGUFãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        self.model_path = model_path
        self.model = None
    
    def load_model(self):
        """å¿…è¦æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.model is None:
            print(f"ğŸ§  LLMãƒ­ãƒ¼ãƒ‰ä¸­: {os.path.basename(self.model_path)}")
            self.model = Llama(
                model_path=self.model_path,
                n_ctx=2048,
                n_gpu_layers=0,
                verbose=False
            )
    
    def unload_model(self):
        """ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ãƒ¢ãƒ‡ãƒ«è§£æ”¾"""
        if self.model:
            del self.model
            self.model = None
            import gc
            gc.collect()
    
    def parse_intent(self, user_input):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è§£æã—ã¦Max/MSPã®æ„å›³ã‚’æŠ½å‡º"""
        self.load_model()
        
        prompt = f"""ã‚ãªãŸã¯Max/MSPã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è¦æ±‚ã‚’åˆ†æã—ã¦JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

è¦æ±‚: "{user_input}"

å›ç­”å½¢å¼:
{{
  "category": "effect|synth|utility|sequencer",
  "subcategory": "å…·ä½“çš„ãªã‚¿ã‚¤ãƒ—",
  "objects": ["å¿…è¦ãªMax/MSPã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"],
  "parameters": {{"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼å": "å€¤"}},
  "complexity": 1-5
}}

JSON:"""
        
        try:
            response = self.model(
                prompt,
                max_tokens=300,
                stop=["\\n\\n", "```"],
                echo=False,
                temperature=0.1
            )
            
            result = response['choices'][0]['text'].strip()
            
            # JSONã‚’æŠ½å‡º
            if '{' in result and '}' in result:
                start = result.find('{')
                end = result.rfind('}') + 1
                json_str = result[start:end]
                return json.loads(json_str)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè§£æ
                return self._fallback_parse(user_input)
                
        except Exception as e:
            print(f"âš ï¸ LLMè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self._fallback_parse(user_input)
    
    def _fallback_parse(self, user_input):
        """LLMå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ"""
        keywords = {
            'reverb': {'category': 'effect', 'subcategory': 'reverb', 'objects': ['adc~', 'freeverb~', 'dac~']},
            'delay': {'category': 'effect', 'subcategory': 'delay', 'objects': ['adc~', 'tapin~', 'tapout~', 'dac~']},
            'filter': {'category': 'effect', 'subcategory': 'filter', 'objects': ['adc~', 'lores~', 'dac~']},
            'oscillator': {'category': 'synth', 'subcategory': 'oscillator', 'objects': ['phasor~', 'dac~']},
            'synth': {'category': 'synth', 'subcategory': 'basic', 'objects': ['phasor~', '*~', 'dac~']}
        }
        
        user_lower = user_input.lower()
        for keyword, config in keywords.items():
            if keyword in user_lower:
                config['complexity'] = 2
                config['parameters'] = {}
                return config
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return {
            'category': 'effect',
            'subcategory': 'basic',
            'objects': ['adc~', 'dac~'],
            'parameters': {},
            'complexity': 1
        }

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    llm = MaxPatchLLM()
    
    test_inputs = [
        "ã‚¹ãƒ†ãƒ¬ã‚ªãƒªãƒãƒ¼ãƒ–ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚’ä½œã£ã¦",
        "FMåˆæˆã§ãƒ™ãƒ«éŸ³ã‚’ä½œã‚ŠãŸã„",
        "ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ"
    ]
    
    for test_input in test_inputs:
        print(f"\\nğŸ“ ãƒ†ã‚¹ãƒˆ: {test_input}")
        result = llm.parse_intent(test_input)
        print(f"ğŸ“Š çµæœ: {result}")
    
    llm.unload_model()
    print("\\nâœ… LLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†!")
'''
    
    with open("/Users/mymac/manxo/scripts/max_patch_llm.py", "w") as f:
        f.write(interface_code)
    
    print("ğŸ“ LLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆå®Œäº†: scripts/max_patch_llm.py")

if __name__ == "__main__":
    print("ğŸš€ å®Ÿç”¨çš„LLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
    
    # Step 1: Qwenã‚’è©¦ã™
    model_path = find_and_download_qwen()
    
    # Step 2: å¤±æ•—ã—ãŸã‚‰ä»£æ›¿ãƒ¢ãƒ‡ãƒ«
    if not model_path:
        print("ğŸ”„ ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
        model_path = download_alternative_model()
    
    # Step 3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    if model_path and test_any_model(model_path):
        print("\\nğŸ‰ LLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆåŠŸ!")
        
        # Step 4: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ
        create_llm_interface()
        
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
        print("ğŸ“ ä½¿ç”¨æ–¹æ³•:")
        print("  python scripts/max_patch_llm.py")
        
    else:
        print("\\nâŒ å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¤±æ•—ã—ã¾ã—ãŸ")