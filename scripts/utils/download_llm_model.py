#!/usr/bin/env python3
"""
13B LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""

import os
from huggingface_hub import hf_hub_download

def download_qwen_model():
    """Qwen2.5-14B INT4é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    
    model_dir = "/Users/mymac/manxo/models/llm"
    os.makedirs(model_dir, exist_ok=True)
    
    print("Qwen2.5-14B INT4é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print("ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç´„8.5GB")
    print("äºˆæƒ³æ™‚é–“: 10-20åˆ†")
    
    try:
        # Qwen2.5-14B-Instruct GGUFå½¢å¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        model_path = hf_hub_download(
            repo_id="Qwen/Qwen2.5-14B-Instruct-GGUF",
            filename="qwen2.5-14b-instruct-q4_k_m.gguf",
            local_dir=model_dir,
            local_dir_use_symlinks=False  # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½¿ã‚ãªã„
        )
        
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {model_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        file_size = os.path.getsize(model_path) / (1024**3)  # GB
        print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f} GB")
        
        return model_path
        
    except Exception as e:
        print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ¥ã®é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
        print("\nğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ...")
        try:
            model_path = hf_hub_download(
                repo_id="Qwen/Qwen2.5-7B-Instruct-GGUF", 
                filename="qwen2.5-7b-instruct-q4_k_m.gguf",
                local_dir=model_dir,
                local_dir_use_symlinks=False
            )
            print(f"âœ… 7Bãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {model_path}")
            return model_path
            
        except Exception as e2:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e2}")
            return None

def test_model(model_path):
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    if not model_path or not os.path.exists(model_path):
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    try:
        from llama_cpp import Llama
        
        print("ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = Llama(
            model_path=model_path,
            n_ctx=512,  # å°ã•ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
            n_gpu_layers=0,  # CPUä½¿ç”¨
            verbose=False
        )
        
        print("ğŸ’¬ ãƒ†ã‚¹ãƒˆç”Ÿæˆä¸­...")
        response = model(
            "Max/MSPã§ãƒªãƒãƒ¼ãƒ–ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚’ä½œã‚‹ã«ã¯ï¼Ÿ",
            max_tokens=100,
            stop=["ã€‚", "\n\n"],
            echo=False
        )
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
        print(f"ğŸ“ å¿œç­”: {response['choices'][0]['text']}")
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del model
        import gc
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ 13B LLMãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹...")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    model_path = download_qwen_model()
    
    if model_path:
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
        if test_model(model_path):
            print("\nğŸ‰ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!")
            print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
        else:
            print("\nâš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯æˆåŠŸã—ã¾ã—ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")